{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./density_estimation_cifar10_A/classification_mnist126/model.ckpt\n",
      "accs:  [0.923  0.9211 0.9199 0.9195 0.9244 0.9207 0.9203 0.922  0.9236 0.9182]\n",
      "0 Final Testing Accuracy:  0.9415\n",
      "TIM:\n",
      "maxp_OOD: 0.0667 0.6330205 0.20062934\n",
      "maxp_inD: 0.7765 0.94906837 0.12320748\n",
      "ent_OOD: 0.8959903 0.43462014 ent_in: 0.12493621 0.2760778\n",
      "MI_OOD: 0.6842741 0.3456992 ent_in: 0.096149355 0.2173931\n",
      "AUPR_p: 90.73\n",
      "AUROC_p: 92.66\n",
      "FPR95: 24.97\n",
      "AUPR_entropy: 92.27\n",
      "AUROC_entropy: 93.59\n",
      "FPR95: 24.77\n",
      "AUPR_MI: 91.77\n",
      "AUROC_MI: 93.26\n",
      "FPR95: 24.959999999999997\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "from utilities import *\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "# SVHN data\n",
    "from scipy import io\n",
    "data1=io.loadmat('train_32x32.mat')\n",
    "train_data=data1['X']\n",
    "train_data = train_data.astype('float32')\n",
    "train_data = np.transpose(train_data, (3, 0, 1, 2))\n",
    "SVHN = train_data/255.\n",
    "\n",
    "# LSUN data\n",
    "LSUN = np.load(\"./LSUN.npy\")\n",
    "lSUN = LSUN/255.\n",
    "\n",
    "# TIM data\n",
    "TIM = np.load(\"./TIM.npy\")  \n",
    "\n",
    "batch_size = 128\n",
    "tf.reset_default_graph()\n",
    "networks = ['network1', 'network2', 'network3', 'network4', 'network5','network6', 'network7', 'network8', 'network9', 'network10']\n",
    "\n",
    "XX_list={}\n",
    "YY_list={}\n",
    "for mm in range(len(networks)):\n",
    "  with tf.name_scope(networks[mm]):\n",
    "    XX_list[networks[mm]] = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = networks[mm] +'x')\n",
    "    YY_list[networks[mm]] = tf.placeholder(tf.float32, [None,10],name =networks[mm] +'y')\n",
    "    \n",
    "X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'x')\n",
    "Dcon = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'Dcon')\n",
    "Y = tf.placeholder(tf.float32, [None,10],name ='y')\n",
    "b = tf.placeholder(tf.bool,shape=(),name='b')\n",
    "learning_rate = tf.placeholder(tf.float32,shape=(),name= 'learning_rate')\n",
    "alpha = tf.placeholder(tf.float32,shape=(),name='alpha')\n",
    "stop_grad = tf.placeholder(tf.bool,shape=(),name='stop')\n",
    "\n",
    "x_list = []\n",
    "y_list = []\n",
    "output_list = []\n",
    "Eent_noise = 0\n",
    "output1 = 0\n",
    "pp = 0\n",
    "for i in range(len(networks)):\n",
    "    x_image, y_label, output, probs,_,_= vgg16(networks[i],False,XX_list,YY_list,1e-6)\n",
    "    _, output_noise = vgg16_1(Dcon,networks[i],False,stop_grad,XX_list,YY_list)\n",
    "    _, _, output_train, probs_train,ent_train,loss= vgg16(networks[i],True,XX_list,YY_list,1e-6)\n",
    "    x_list.append(x_image)\n",
    "    y_list.append(y_label)\n",
    "    output_list.append(output)\n",
    "    pp+=loss\n",
    "    Eent_noise += tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "    output1 += output_noise\n",
    "    \n",
    "output1 = output1/len(networks)\n",
    "ent_noise = tf.reduce_sum(-tf.log(output1+1e-30)*output1,1)\n",
    "Eent_noise = Eent_noise/len(networks)\n",
    "MI = ent_noise - Eent_noise\n",
    "MI_mean=tf.reduce_mean(MI)\n",
    "    \n",
    "pp = pp/len(networks)\n",
    "loss = pp-MI_mean*alpha\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_id=0\n",
    "save_path = saver.restore(sess, \"./density_estimation_cifar10_A/classification_mnist126/model.ckpt\" )\n",
    "epoch = 391\n",
    "num_iter = epoch*200\n",
    "for iter in range(1):\n",
    "    if iter%1000 == 0:\n",
    "        outputs=[]\n",
    "        x_test_batch = np.copy(x_test)\n",
    "        y_test_batch = np.copy(y_test)\n",
    "        x_test_batch = stand(x_test_batch)\n",
    "        for i in range(len(networks)):\n",
    "            probs = []\n",
    "            for j in range(10):   \n",
    "              prob= sess.run(output_list[i], {x_list[i]:x_test_batch[j*1000:(j+1)*1000], y_list[i]:y_test_batch[j*1000:(j+1)*1000], b:False})\n",
    "              probs.extend(prob)  \n",
    "            outputs.append(probs)\n",
    "        outputs=np.array(outputs) \n",
    "        \n",
    "        accs = np.mean(np.argmax(outputs,2)==np.argmax(y_test_batch,1),1)\n",
    "        print(\"accs: \",accs)\n",
    "\n",
    "        softmax = np.mean(outputs,0)\n",
    "        maxp_in = np.max(softmax,1)\n",
    "        acc = np.mean(np.argmax(softmax,1) ==  np.argmax(y_test_batch,1))\n",
    "        print( iter, 'Final Testing Accuracy: ', acc)\n",
    "        \n",
    "        ent_in = np.sum(-np.log(softmax+1e-11)*softmax,1)\n",
    "        Eent_in = np.mean(np.sum(-np.log(outputs+1e-11)*outputs,2),0)\n",
    "        MI_in = ent_in - Eent_in\n",
    "        \n",
    "        right = np.argmax(softmax,1) ==  np.argmax(y_test_batch,1)\n",
    "        wrong_data0 = x_test_batch[~right]\n",
    "        wrong_data = wrong_data0[np.random.randint(0,len(wrong_data0),10000)]\n",
    "        right_data = x_test_batch[right]\n",
    "        right_data = right_data[np.random.randint(0,len(right_data),10000)]\n",
    "        \n",
    "        outputs_right=[]\n",
    "        for i in range(len(networks)):\n",
    "            prob= sess.run(output_list[i], {x_list[i]:right_data, b:False})\n",
    "            outputs_right.append(prob)\n",
    "        outputs_right=np.array(outputs_right) \n",
    "        softmax_right = np.mean(outputs_right,0)\n",
    "        maxp_in_right = np.max(softmax_right,1)\n",
    "        ent_in_right = np.sum(-np.log(softmax_right+1e-11)*softmax_right,1)\n",
    "        Eent_in_right = np.mean(np.sum(-np.log(outputs_right+1e-11)*outputs_right,2),0)\n",
    "        MI_in_right = ent_in_right - Eent_in_right\n",
    "\n",
    "        safe_images = TIM[np.random.randint(0,10000,10000)]\n",
    "        print(\"TIM:\")         \n",
    "        safe_images = stand(safe_images)   \n",
    "\n",
    "        outputs_OOD=[]\n",
    "        for j in range(len(networks)):\n",
    "          probs_OOD  = []\n",
    "          for r in range(10):\n",
    "            prob_OOD = sess.run(output_list[j], {x_list[j]:safe_images[r*1000:(r+1)*1000], b:False})\n",
    "            probs_OOD.extend(prob_OOD)  \n",
    "          outputs_OOD.append(probs_OOD)\n",
    "        outputs_OOD = np.array(outputs_OOD) \n",
    "\n",
    "        softmax_OOD = np.mean(outputs_OOD,0)\n",
    "        maxp_OOD = np.max(softmax_OOD,1)\n",
    "        ent_OOD = np.sum(-np.log(softmax_OOD+1e-11)*softmax_OOD,1)\n",
    "        Eent_OOD = np.mean(np.sum(-np.log(outputs_OOD+1e-11)*outputs_OOD,2),0)\n",
    "        MI_OOD = ent_OOD - Eent_OOD\n",
    "\n",
    "        print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "        print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "        print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "        print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"ent_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "\n",
    "        safe, risky  = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "        print(\"############################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We choose the threshold to be 0.24024024024024024 which yields the best OOD detection accuracy:  0.86035\n",
      "The precision, recall and F1 score are  0.8924170089930055 0.8038 0.845793654969222 respectively.\n",
      "The model thinks there are  9007.0 in distributional data out of the 20000 data points.\n"
     ]
    }
   ],
   "source": [
    "safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "best = 0\n",
    "threshhold = 0\n",
    "for thresh in np.linspace(0,1,1000):\n",
    "    tmp = np.mean((1-(examples<thresh))==labels)\n",
    "    if tmp>=best:\n",
    "        best = tmp\n",
    "        threshhold = thresh\n",
    "print(\"We choose the threshold to be\",threshhold,\"which yields the best OOD detection accuracy: \",best)\n",
    "\n",
    "ioo = (examples<threshhold)*1.\n",
    "row = np.concatenate((right,(1-right)*0))\n",
    "np.mean(ioo*row)\n",
    "p = np.sum(ioo*row)/np.sum(ioo)\n",
    "r = np.sum(ioo*row)/10000\n",
    "f1 = 2*p*r/(p+r)\n",
    "print(\"The precision, recall and F1 score are \",p,r,f1,\"respectively.\")\n",
    "print(\"The model thinks there are \",np.sum(ioo),\"in distributional data out of the 20000 data points.\")\n",
    "\n",
    "right1 = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./density_estimation_cifar10_A/classification_mnist666097/model.ckpt\n",
      "accs:  [0.8833 0.8398 0.8752 0.8933 0.881  0.8818 0.8734 0.8327 0.9094 0.9055]\n",
      "0 Final Testing Accuracy:  0.9274\n",
      "TIM:\n",
      "maxp_OOD: 0.0032 0.476109 0.1873872\n",
      "maxp_inD: 0.4436 0.8900021 0.16599825\n",
      "ent_OOD: 1.40956 0.43491194 ent_in: 0.3204154 0.40812796\n",
      "MI_OOD: 0.8702489 0.3754089 ent_in: 0.16668713 0.23101398\n",
      "AUPR_p: 92.58\n",
      "AUROC_p: 93.39\n",
      "FPR95: 24.46\n",
      "AUPR_entropy: 94.48\n",
      "AUROC_entropy: 94.88\n",
      "FPR95: 22.39\n",
      "AUPR_MI: 94.6\n",
      "AUROC_MI: 94.82\n",
      "FPR95: 23.93\n",
      "############################################\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "from utilities import *\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "# SVHN data\n",
    "from scipy import io\n",
    "data1=io.loadmat('train_32x32.mat')\n",
    "train_data=data1['X']\n",
    "train_data = train_data.astype('float32')\n",
    "train_data = np.transpose(train_data, (3, 0, 1, 2))\n",
    "SVHN = train_data/255.\n",
    "\n",
    "# LSUN data\n",
    "LSUN = np.load(\"./LSUN.npy\")\n",
    "lSUN = LSUN/255.\n",
    "\n",
    "# TIM data\n",
    "TIM = np.load(\"./TIM.npy\")  \n",
    "\n",
    "batch_size = 128\n",
    "tf.reset_default_graph()\n",
    "networks = ['network1', 'network2', 'network3', 'network4', 'network5','network6', 'network7', 'network8', 'network9', 'network10']\n",
    "\n",
    "XX_list={}\n",
    "YY_list={}\n",
    "for mm in range(len(networks)):\n",
    "  with tf.name_scope(networks[mm]):\n",
    "    XX_list[networks[mm]] = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = networks[mm] +'x')\n",
    "    YY_list[networks[mm]] = tf.placeholder(tf.float32, [None,10],name =networks[mm] +'y')\n",
    "    \n",
    "X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'x')\n",
    "Dcon = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'Dcon')\n",
    "Y = tf.placeholder(tf.float32, [None,10],name ='y')\n",
    "b = tf.placeholder(tf.bool,shape=(),name='b')\n",
    "learning_rate = tf.placeholder(tf.float32,shape=(),name= 'learning_rate')\n",
    "alpha = tf.placeholder(tf.float32,shape=(),name='alpha')\n",
    "stop_grad = tf.placeholder(tf.bool,shape=(),name='stop')\n",
    "\n",
    "x_list = []\n",
    "y_list = []\n",
    "output_list = []\n",
    "Eent_noise = 0\n",
    "output1 = 0\n",
    "pp = 0\n",
    "for i in range(len(networks)):\n",
    "    x_image, y_label, output, probs,_,_= vgg16(networks[i],False,XX_list,YY_list,1e-6)\n",
    "    _, output_noise = vgg16_1(Dcon,networks[i],False,stop_grad,XX_list,YY_list)\n",
    "    _, _, output_train, probs_train,ent_train,loss= vgg16(networks[i],True,XX_list,YY_list,1e-6)\n",
    "    x_list.append(x_image)\n",
    "    y_list.append(y_label)\n",
    "    output_list.append(output)\n",
    "    pp+=loss\n",
    "    Eent_noise += tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "    output1 += output_noise\n",
    "    \n",
    "output1 = output1/len(networks)\n",
    "ent_noise = tf.reduce_sum(-tf.log(output1+1e-30)*output1,1)\n",
    "Eent_noise = Eent_noise/len(networks)\n",
    "MI = ent_noise - Eent_noise\n",
    "MI_mean=tf.reduce_mean(MI)\n",
    "    \n",
    "pp = pp/len(networks)\n",
    "loss = pp-MI_mean*alpha\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_id=0\n",
    "save_path = saver.restore(sess,\"./density_estimation_cifar10_A/classification_mnist666097/model.ckpt\" )\n",
    "epoch = 391\n",
    "num_iter = epoch*200\n",
    "for iter in range(1):\n",
    "\n",
    "    if iter%1000 == 0:\n",
    "        outputs=[]\n",
    "        x_test_batch = np.copy(x_test)\n",
    "        y_test_batch = np.copy(y_test)\n",
    "        x_test_batch = stand(x_test_batch)\n",
    "        for i in range(len(networks)):\n",
    "            probs = []\n",
    "            for j in range(10):   \n",
    "              prob= sess.run(output_list[i], {x_list[i]:x_test_batch[j*1000:(j+1)*1000], y_list[i]:y_test_batch[j*1000:(j+1)*1000], b:False})\n",
    "              probs.extend(prob)  \n",
    "            outputs.append(probs)\n",
    "        outputs=np.array(outputs) \n",
    "        \n",
    "        accs = np.mean(np.argmax(outputs,2)==np.argmax(y_test_batch,1),1)\n",
    "        print(\"accs: \",accs)\n",
    "\n",
    "        softmax = np.mean(outputs,0)\n",
    "        maxp_in = np.max(softmax,1)\n",
    "        acc = np.mean(np.argmax(softmax,1) ==  np.argmax(y_test_batch,1))\n",
    "        print( iter, 'Final Testing Accuracy: ', acc)\n",
    "        \n",
    "        ent_in = np.sum(-np.log(softmax+1e-11)*softmax,1)\n",
    "        Eent_in = np.mean(np.sum(-np.log(outputs+1e-11)*outputs,2),0)\n",
    "        MI_in = ent_in - Eent_in\n",
    "        \n",
    "        right = np.argmax(softmax,1) ==  np.argmax(y_test_batch,1)\n",
    "        wrong_data0 = x_test_batch[~right]\n",
    "        wrong_data = wrong_data0[np.random.randint(0,len(wrong_data0),10000)]\n",
    "        right_data = x_test_batch[right]\n",
    "        right_data = right_data[np.random.randint(0,len(right_data),10000)]\n",
    "        \n",
    "        outputs_right=[]\n",
    "        for i in range(len(networks)):\n",
    "            prob= sess.run(output_list[i], {x_list[i]:right_data, b:False})\n",
    "            outputs_right.append(prob)\n",
    "        outputs_right=np.array(outputs_right) \n",
    "        softmax_right = np.mean(outputs_right,0)\n",
    "        maxp_in_right = np.max(softmax_right,1)\n",
    "        ent_in_right = np.sum(-np.log(softmax_right+1e-11)*softmax_right,1)\n",
    "        Eent_in_right = np.mean(np.sum(-np.log(outputs_right+1e-11)*outputs_right,2),0)\n",
    "        MI_in_right = ent_in_right - Eent_in_right\n",
    "\n",
    "        safe_images = TIM[np.random.randint(0,10000,10000)]\n",
    "        print(\"TIM:\")\n",
    "        safe_images = stand(safe_images)   \n",
    "\n",
    "        outputs_OOD=[]\n",
    "        for j in range(len(networks)):\n",
    "          probs_OOD  = []\n",
    "          for r in range(10):\n",
    "            prob_OOD = sess.run(output_list[j], {x_list[j]:safe_images[r*1000:(r+1)*1000], b:False})\n",
    "            probs_OOD.extend(prob_OOD)  \n",
    "          outputs_OOD.append(probs_OOD)\n",
    "        outputs_OOD = np.array(outputs_OOD) \n",
    "\n",
    "        softmax_OOD = np.mean(outputs_OOD,0)\n",
    "        maxp_OOD = np.max(softmax_OOD,1)\n",
    "        ent_OOD = np.sum(-np.log(softmax_OOD+1e-11)*softmax_OOD,1)\n",
    "        Eent_OOD = np.mean(np.sum(-np.log(outputs_OOD+1e-11)*outputs_OOD,2),0)\n",
    "        MI_OOD = ent_OOD - Eent_OOD\n",
    "\n",
    "        print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "        print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "        print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "        print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"ent_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "\n",
    "        safe, risky  = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "        print(\"############################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We choose the threshold to be 0.7797797797797797 which yields the best OOD detection accuracy:  0.87885\n",
      "The precision, recall and F1 score are  0.8827180310326378 0.8249 0.8528301886792452 respectively.\n",
      "The model thinks there are  9345.0 in distributional data out of the 20000 data points.\n"
     ]
    }
   ],
   "source": [
    "safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "best = 0\n",
    "threshhold = 0\n",
    "for thresh in np.linspace(0,1,1000):\n",
    "    tmp = np.mean((1-(examples<thresh))==labels)\n",
    "    if tmp>=best:\n",
    "        best = tmp\n",
    "        threshhold = thresh\n",
    "print(\"We choose the threshold to be\",threshhold,\"which yields the best OOD detection accuracy: \",best)\n",
    "\n",
    "ioo = (examples<threshhold)*1.\n",
    "row = np.concatenate((right,(1-right)*0))\n",
    "np.mean(ioo*row)\n",
    "p = np.sum(ioo*row)/np.sum(ioo)\n",
    "r = np.sum(ioo*row)/10000\n",
    "f1 = 2*p*r/(p+r)\n",
    "print(\"The precision, recall and F1 score are \",p,r,f1,\"respectively.\")\n",
    "print(\"The model thinks there are \",np.sum(ioo),\"in distributional data out of the 20000 data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Combined System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision, recall and F1 score are  0.8868913857677903 0.8288 0.8568622383044714 respectively.\n",
      "The model thinks there are  9345.0 in distributional data out of the 20000 data points.\n"
     ]
    }
   ],
   "source": [
    "ioo = (examples<threshhold)*1.\n",
    "row = np.concatenate((right1,(1-right1)*0))\n",
    "np.mean(ioo*row)\n",
    "p = np.sum(ioo*row)/np.sum(ioo)\n",
    "r = np.sum(ioo*row)/10000\n",
    "f1 = 2*p*r/(p+r)\n",
    "print(\"The precision, recall and F1 score are \",p,r,f1,\"respectively.\")\n",
    "print(\"The model thinks there are \",np.sum(ioo),\"in distributional data out of the 20000 data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 achieves high precision but low recall because it filters out too many in-distributional samples, resulting in a low F1 score; Model 2 achieves higher recall and better F1 score because it knows better about what is in-distributional and what is OOD; the Combined System achieves the best F1 score because it gets benefit from both Model 1 and Model 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
