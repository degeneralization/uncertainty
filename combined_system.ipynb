{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/senqi/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "./density_estimation_cifar10_A/classification_mnist126; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06cf3eb3360e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./density_estimation_cifar10_A/classification_mnist126/model.ckpt\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m391\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m   1278\u001b[0m                        compat.as_text(save_path))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    370\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    371\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 384\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    385\u001b[0m     ]\n\u001b[1;32m    386\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ./density_estimation_cifar10_A/classification_mnist126; No such file or directory"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io\n",
    "import random\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "from utilities import *\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# cifar10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "# SVHN data\n",
    "from scipy import io\n",
    "data1=io.loadmat('train_32x32.mat')\n",
    "train_data=data1['X']\n",
    "train_data = train_data.astype('float32')\n",
    "train_data = np.transpose(train_data, (3, 0, 1, 2))\n",
    "SVHN = train_data/255.\n",
    "\n",
    "# LSUN data\n",
    "LSUN = np.load(\"./LSUN.npy\")\n",
    "lSUN = LSUN/255.\n",
    "\n",
    "# TIM data\n",
    "TIM = np.load(\"./TIM.npy\")  \n",
    "\n",
    "batch_size = 128\n",
    "tf.reset_default_graph()\n",
    "networks = ['network1', 'network2', 'network3', 'network4', 'network5','network6', 'network7', 'network8', 'network9', 'network10']\n",
    "\n",
    "XX_list={}\n",
    "YY_list={}\n",
    "for mm in range(len(networks)):\n",
    "  with tf.name_scope(networks[mm]):\n",
    "    XX_list[networks[mm]] = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = networks[mm] +'x')\n",
    "    YY_list[networks[mm]] = tf.placeholder(tf.float32, [None,10],name =networks[mm] +'y')\n",
    "    \n",
    "X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'x')\n",
    "Dcon = tf.placeholder(tf.float32, shape = [None, 32, 32, 3],name = 'Dcon')\n",
    "Y = tf.placeholder(tf.float32, [None,10],name ='y')\n",
    "b = tf.placeholder(tf.bool,shape=(),name='b')\n",
    "learning_rate = tf.placeholder(tf.float32,shape=(),name= 'learning_rate')\n",
    "alpha = tf.placeholder(tf.float32,shape=(),name='alpha')\n",
    "stop_grad = tf.placeholder(tf.bool,shape=(),name='stop')\n",
    "\n",
    "x_list = []\n",
    "y_list = []\n",
    "output_list = []\n",
    "Eent_noise = 0\n",
    "output1 = 0\n",
    "pp = 0\n",
    "for i in range(len(networks)):\n",
    "    x_image, y_label, output, probs,_,_= vgg16(networks[i],False,XX_list,YY_list,1e-6)\n",
    "    _, output_noise = vgg16_1(Dcon,networks[i],False,stop_grad,XX_list,YY_list)\n",
    "    _, _, output_train, probs_train,ent_train,loss= vgg16(networks[i],True,XX_list,YY_list,1e-6)\n",
    "    x_list.append(x_image)\n",
    "    y_list.append(y_label)\n",
    "    output_list.append(output)\n",
    "    pp+=loss\n",
    "    Eent_noise += tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "    output1 += output_noise\n",
    "    \n",
    "output1 = output1/len(networks)\n",
    "ent_noise = tf.reduce_sum(-tf.log(output1+1e-30)*output1,1)\n",
    "Eent_noise = Eent_noise/len(networks)\n",
    "MI = ent_noise - Eent_noise\n",
    "MI_mean=tf.reduce_mean(MI)\n",
    "    \n",
    "pp = pp/len(networks)\n",
    "loss = pp-MI_mean*alpha\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "model_id=0\n",
    "save_path = saver.restore(sess, \"./density_estimation_cifar10_A/classification_mnist126/model.ckpt\" )\n",
    "epoch = 391\n",
    "num_iter = epoch*200\n",
    "for iter in range(1):\n",
    "    if iter%1000 == 0:\n",
    "        outputs=[]\n",
    "        x_test_batch = np.copy(x_test)\n",
    "        y_test_batch = np.copy(y_test)\n",
    "        x_test_batch = stand(x_test_batch)\n",
    "        for i in range(len(networks)):\n",
    "            probs = []\n",
    "            for j in range(10):   \n",
    "              prob= sess.run(output_list[i], {x_list[i]:x_test_batch[j*1000:(j+1)*1000], y_list[i]:y_test_batch[j*1000:(j+1)*1000], b:False})\n",
    "              probs.extend(prob)  \n",
    "            outputs.append(probs)\n",
    "        outputs=np.array(outputs) \n",
    "        \n",
    "        accs = np.mean(np.argmax(outputs,2)==np.argmax(y_test_batch,1),1)\n",
    "        print(\"accs: \",accs)\n",
    "\n",
    "        softmax = np.mean(outputs,0)\n",
    "        maxp_in = np.max(softmax,1)\n",
    "        acc = np.mean(np.argmax(softmax,1) ==  np.argmax(y_test_batch,1))\n",
    "        print( iter, 'Final Testing Accuracy: ', acc)\n",
    "        \n",
    "        ent_in = np.sum(-np.log(softmax+1e-11)*softmax,1)\n",
    "        Eent_in = np.mean(np.sum(-np.log(outputs+1e-11)*outputs,2),0)\n",
    "        MI_in = ent_in - Eent_in\n",
    "        \n",
    "        right = np.argmax(softmax,1) ==  np.argmax(y_test_batch,1)\n",
    "        wrong_data0 = x_test_batch[~right]\n",
    "        wrong_data = wrong_data0[np.random.randint(0,len(wrong_data0),10000)]\n",
    "        right_data = x_test_batch[right]\n",
    "        right_data = right_data[np.random.randint(0,len(right_data),10000)]\n",
    "        \n",
    "        outputs_right=[]\n",
    "        for i in range(len(networks)):\n",
    "            prob= sess.run(output_list[i], {x_list[i]:right_data, b:False})\n",
    "            outputs_right.append(prob)\n",
    "        outputs_right=np.array(outputs_right) \n",
    "        softmax_right = np.mean(outputs_right,0)\n",
    "        maxp_in_right = np.max(softmax_right,1)\n",
    "        ent_in_right = np.sum(-np.log(softmax_right+1e-11)*softmax_right,1)\n",
    "        Eent_in_right = np.mean(np.sum(-np.log(outputs_right+1e-11)*outputs_right,2),0)\n",
    "        MI_in_right = ent_in_right - Eent_in_right\n",
    "\n",
    "        safe_images = TIM[np.random.randint(0,10000,10000)]\n",
    "        print(\"TIM:\")         \n",
    "        safe_images = stand(safe_images)   \n",
    "\n",
    "        outputs_OOD=[]\n",
    "        for j in range(len(networks)):\n",
    "          probs_OOD  = []\n",
    "          for r in range(10):\n",
    "            prob_OOD = sess.run(output_list[j], {x_list[j]:safe_images[r*1000:(r+1)*1000], b:False})\n",
    "            probs_OOD.extend(prob_OOD)  \n",
    "          outputs_OOD.append(probs_OOD)\n",
    "        outputs_OOD = np.array(outputs_OOD) \n",
    "\n",
    "        softmax_OOD = np.mean(outputs_OOD,0)\n",
    "        maxp_OOD = np.max(softmax_OOD,1)\n",
    "        ent_OOD = np.sum(-np.log(softmax_OOD+1e-11)*softmax_OOD,1)\n",
    "        Eent_OOD = np.mean(np.sum(-np.log(outputs_OOD+1e-11)*outputs_OOD,2),0)\n",
    "        MI_OOD = ent_OOD - Eent_OOD\n",
    "\n",
    "        print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "        print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "        print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "        print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"ent_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "\n",
    "        safe, risky  = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "\n",
    "        safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "        labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "        labels[safe.shape[0]:] += 1\n",
    "        examples = np.squeeze(np.vstack((safe, risky)))\n",
    "        print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "        print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "        print(\"FPR95:\",ErrorRateAt95Recall1(labels, examples)*100)\n",
    "        print(\"############################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "best = 0\n",
    "threshhold = 0\n",
    "for thresh in np.linspace(0,1,1000):\n",
    "    tmp = np.mean((1-(examples<thresh))==labels)\n",
    "    if tmp>=best:\n",
    "        best = tmp\n",
    "        threshhold = thresh\n",
    "print(best,threshhold)\n",
    "\n",
    "\n",
    "best0 = 0\n",
    "threshhold0 = 0\n",
    "for thresh in np.linspace(0,3,1000):\n",
    "    tmp = np.mean(((np.squeeze(safe)<thresh))==right)\n",
    "    if tmp>=best0:\n",
    "        best0 = tmp\n",
    "        threshhold0 = thresh\n",
    "print(best0,threshhold0)\n",
    "\n",
    "\n",
    "ioo = (examples<threshhold)*1.\n",
    "row = np.concatenate((right,(1-right)*0))\n",
    "np.mean(ioo*row)\n",
    "p = np.sum(ioo*row)/np.sum(ioo)\n",
    "r = np.sum(ioo*row)/10000\n",
    "f1 = 2*p*r/(p+r)\n",
    "p,r,f1,np.sum(ioo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
